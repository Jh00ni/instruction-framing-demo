<!--
Interpretation of Monetary Policy Language
Author: Joni H.
Year: 2026
Purpose: Demonstration of NLP model behavior (FinBERT + Llama)
All original code written by the author.
-->

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Interpretation of Monetary Policy Language</title>

    

  <style>
    :root{
      --text:#111;
      --muted:#555;
      --line:#ddd;
      --bg:#fff;
      --panel:#f6f6f6;
      --chip:#f2f2f2;
      --chipBorder:#e0e0e0;
      --accent:#111;
      --highlight:#ffe08a;
    }
    body{
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      background: var(--bg);
      color: var(--text);
      margin: 0;
      padding: 36px 18px;
    }
    .wrap{ max-width: 920px; margin: 0 auto; }
    h1{ font-size: 44px; line-height: 1.1; margin: 0 0 14px; }
    p{ margin: 8px 0; font-size: 18px; color: var(--muted); }
    hr{ border: none; border-top: 1px solid var(--line); margin: 28px 0; }

    .card{
      border: 1px solid var(--line);
      border-radius: 14px;
      padding: 18px 18px 16px;
      margin: 14px 0;
      background: #fff;
    }
    .card h2{
      margin: 0 0 10px;
      font-size: 24px;
      color: var(--text);
    }
    .sentence{
      font-size: 20px;
      line-height: 1.4;
      margin: 10px 0 12px;
    }
    .sentence strong{ color: var(--text); }

    .pill{
      display: inline-block;
      font-size: 12px;
      letter-spacing: .02em;
      padding: 5px 9px;
      border-radius: 999px;
      background: var(--panel);
      border: 1px solid var(--line);
      color: var(--muted);
      margin-bottom: 10px;
    }

    .options{
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
      margin: 10px 0 12px;
    }
    .opt{
      border: 1px solid var(--line);
      border-radius: 10px;
      padding: 10px 12px;
      cursor: pointer;
      user-select: none;
      background: #fff;
      font-size: 16px;
    }
    .opt input{ margin-right: 8px; }

    .btn{
      padding: 10px 14px;
      border-radius: 10px;
      border: 1px solid var(--line);
      background: #fff;
      cursor: pointer;
      font-size: 16px;
    }
    .btn:hover{ border-color:#bbb; }

    .reveal{
      display: none;
      margin-top: 14px;
      padding: 14px;
      border-radius: 12px;
      background: var(--panel);
      border: 1px solid var(--line);
    }
    .reveal h3{
      margin: 0 0 8px;
      font-size: 18px;
    }

    .mono{
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace;
      font-size: 14px;
      white-space: pre-wrap;
      line-height: 1.35;
    }
    .note{
      margin-top: 10px;
      font-size: 14px;
      color: var(--muted);
    }

    /* Token “chips” */
    .tokenWrap{
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      margin-top: 10px;
    }
    .tok{
      padding: 7px 10px;
      border-radius: 999px;
      background: var(--chip);
      border: 1px solid var(--chipBorder);
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace;
      font-size: 14px;
      color: var(--text);
    }
    .tok.hl{
      background: var(--highlight);
      border-color: #e3b84c;
    }
    .tok.dim{ opacity: .85; }

    /* Side-by-side panels */
    .grid2{
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 12px;
    }
    @media (max-width: 800px){
      h1{ font-size: 34px; }
      .grid2{ grid-template-columns: 1fr; }
    }
    .panel{
      border: 1px solid var(--line);
      border-radius: 12px;
      padding: 12px;
      background: #fff;
    }
    .panel .title{
      font-weight: 700;
      margin-bottom: 8px;
    }

    .small{ font-size: 15px; color: var(--muted); }

    .kpi{
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
      margin-top: 8px;
    }
    .kpi span{
      display: inline-block;
      border: 1px solid var(--line);
      background: #fff;
      border-radius: 10px;
      padding: 8px 10px;
      font-size: 15px;
    }
    .kpi span.best{
      border-color:#111;
      box-shadow: 0 0 0 2px rgba(0,0,0,.08);
      font-weight: 700;
    }

    /* Prompt blocks */
    .promptBlock{
      border: 1px solid var(--line);
      border-radius: 12px;
      background: #fff;
      padding: 12px;
    }
    .promptHdr{
      display:flex;
      justify-content: space-between;
      align-items:center;
      gap: 10px;
      margin-bottom: 8px;
    }
    .promptHdr .tag{
      display:inline-block;
      font-size: 12px;
      padding: 4px 8px;
      border-radius: 999px;
      border: 1px solid var(--line);
      background: var(--panel);
      color: var(--muted);
      white-space: nowrap;
    }
    .divider{
      height: 1px;
      background: var(--line);
      margin: 10px 0;
    }
    .sectionPart{
  padding-left: 18px;   /* same as .card horizontal padding */
  padding-right: 18px;
  margin: 28px 0 18px;
}
/* Embeddings table */
.tbl{
  width: 100%;
  border-collapse: collapse;
  margin-top: 10px;
  background: #fff;
  border: 1px solid var(--line);
  border-radius: 12px;
  overflow: hidden;
}
.tbl th, .tbl td{
  text-align: left;
  padding: 10px 12px;
  border-top: 1px solid var(--line);
  font-size: 14px;
}
.tbl th{
  background: var(--panel);
  font-weight: 700;
}
.tbl tr:first-child td{ border-top: none; }
.tbl code{
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace;
  font-size: 13px;
}
.tbl .hlRow{
  background: #fff6d1;
}
.explain{
  font-size: 15px;      /* matches .small */
  color: var(--muted);  /* matches .small */
  line-height: 1.45;
  margin: 10px 0 8px;
}
.explain strong{
  color: var(--text);
  font-weight: 700;
}
#flow_block li {
  margin-bottom: 6px;
}

  </style>

  <!-- Plotly for the attention heatmap -->
  <script src="https://cdn.plot.ly/plotly-2.30.0.min.js"></script>
</head>

<body>
<div class="wrap">
  <h1>Interpretation of Monetary Policy Language (Made by Joni H.)</h1>
  <br>
  <p>Stylized examples. No responses are stored. Click <strong>“Reveal”</strong> to compare with model outputs.</p>

<hr>

<h2 style="margin-top:10px;">Part I — Language Model (Financial Sentiment Classifier (FinBERT))</h2>
<p class="small" style="margin-top:4px;">
The following examples use a fine-tuned FinBERT (110 M parameters).<br>
The model outputs probabilistic classifications: <strong>Positive</strong>, <strong>Neutral</strong>, or <strong>Negative</strong>.
</p>

  <!-- 1) Human classification: Sentence A -->
<div class="card" id="q1">
  <div class="pill">1/5 — Sentence classification</div>
  <h2>Sentence A:</h2>
  <div class="sentence">"Real GDP changed by 1.0% from last year."</div>

  <div class="small"><strong>Question 1:</strong> What is the tone of this sentence?</div>
  <div class="options">
    <label class="opt"><input type="radio" name="sentA" value="Positive">Positive</label>
    <label class="opt"><input type="radio" name="sentA" value="Neutral">Neutral</label>
    <label class="opt"><input type="radio" name="sentA" value="Negative">Negative</label>
  </div>

  <button class="btn" onclick="reveal('outA')">Reveal model output</button>

  <div class="reveal" id="outA">
    <h3>FinBERT output (precomputed)</h3>
    <div class="kpi">
      <span class="best">Positive: <strong>0.617</strong></span>
      <span>Negative: <strong>0.348</strong></span>
      <span>Neutral : <strong>0.035</strong></span>
    </div>
    <div class="note">Note: probabilities depend on model + tokenizer + preprocessing.</div>
  </div>
</div>

<!-- 2) Human classification: Sentence B -->
<div class="card" id="q2">
  <div class="pill">2/5 — Sentence classification</div>
  <h2>Sentence B:</h2>
  <div class="sentence">"Real GDP changed by <strong>-</strong>1.0% from last year."</div>

  <div class="small"><strong>Question 2:</strong> What is the tone of this sentence?</div>

  <div class="options">
    <label class="opt"><input type="radio" name="sentB" value="Positive">Positive</label>
    <label class="opt"><input type="radio" name="sentB" value="Neutral">Neutral</label>
    <label class="opt"><input type="radio" name="sentB" value="Negative">Negative</label>
  </div>

  <button class="btn" onclick="reveal('outB')">Reveal model output</button>

  <div class="reveal" id="outB">
    <h3>FinBERT output (precomputed)</h3>
    <div class="kpi">
      <span class="best">Negative: <strong>0.593</strong></span>
      <span>Positive: <strong>0.367</strong></span>
      <span>Neutral : <strong>0.040</strong></span>
    </div>
    <div class="note">Only one character differs (“-”), but tokenization can change the input sequence.</div>
  </div>
</div>


<hr>

<h2 style="margin:0 0 6px;">Part II — Under the Hood</h2>
<p class="small" style="margin:0;">
  To understand why a single character can change a prediction, we need to look at how the model represents text internally.
</p>

<!-- 5) Tokens + Attention -->
<div class="card" id="q5">
  <div class="pill">3/5 — What the model sees</div>
  <h2>What does the FinBERT actually see?</h2>

  <div class="small">
    <strong>Question 3:</strong> Why can a single character (like “-” in -1.0%) change the model’s classification?
  </div>

  <br>

  <p class="small">
    Tokenization: The sentence is split into tokens (subwords and symbols). For example, the minus sign “-” becomes its own token.
  </p>

  <div class="sentence"><strong>Tokenization of Sentence B</strong> (“Real GDP changed by -1.0% from last year.”)</div>

  <button class="btn" onclick="reveal('tokOut')">Show tokens</button>

  <div class="reveal" id="tokOut">
    <!-- Replace these chips with your real tokenizer output if you want -->
    <div class="tokenWrap" aria-label="tokens-b">
  <span class="tok dim">[CLS]</span>
  <span class="tok dim">real</span>
  <span class="tok dim">gdp</span>
  <span class="tok dim">changed</span>
  <span class="tok dim">by</span>
  <span class="tok hl">-</span>
  <span class="tok dim">1</span>
  <span class="tok dim">.</span>
  <span class="tok dim">0</span>
  <span class="tok dim">%</span>
  <span class="tok dim">from</span>
  <span class="tok dim">last</span>
  <span class="tok dim">year</span>
  <span class="tok dim">.</span>
  <span class="tok dim">[SEP]</span>
</div>

<hr>

    <div class="sentence"><strong>Tokenization of Sentence A</strong> (“Real GDP changed by 1.0% from last year.”)</div>
    <div class="tokenWrap" aria-label="tokens-a">
  <span class="tok dim">[CLS]</span>
  <span class="tok dim">real</span>
  <span class="tok dim">gdp</span>
  <span class="tok dim">changed</span>
  <span class="tok dim">by</span>
  <span class="tok dim">1</span>
  <span class="tok dim">.</span>
  <span class="tok dim">0</span>
  <span class="tok dim">%</span>
  <span class="tok dim">from</span>
  <span class="tok dim">last</span>
  <span class="tok dim">year</span>
  <span class="tok dim">.</span>
  <span class="tok dim">[SEP]</span>
</div>


<hr>

<h3>Embeddings (meaning in numbers)</h3>

<p class="explain">
  The model turns each token into a long list of numbers (here: 768 numbers).
  These numbers place the token in a “meaning space”, where similar words end up near each other.
</p>


<button class="btn" onclick="toggleEmbeddings()">Show embeddings</button>

<div id="emb_block" class="reveal" style="display:none;">
  <h3 style="margin: 0 0 8px;">Token embeddings (first 4 of 768 dimensions)</h3>
  <p class="small" style="margin-top:0;">
    Each token is mapped to a 768-dimensional numeric vector. Below show only the first 4 values.
  </p>
  <div class="small" id="emb_meta"></div>
  <div id="emb_table"></div>
  <div class="note">Highlighted row shows the <code>-</code> token.</div>
</div>


<hr>
<h3>Attention (heatmap)</h3>
<p class="explain">
  <strong>What this shows:</strong>
  how the model “looks around” at other tokens to decide what is important for understanding the sentence.
</p>
    <p class="small" style="margin-top:0;">
      FinBERT has 12 transformer layers with 12 attention heads per layer. This visualization shows one head from the final layer.
      Different heads specialize in different structural or semantic patterns, so this is only a partial view of the model’s internal dynamics.
    </p>

    <div class="sentence"><strong>Sentence:</strong> <span id="att_sentence"></span></div>

    <button class="btn" onclick="toggleAttention()">Reveal attention heatmap</button>

    <div id="att_block" class="reveal" style="display:none;">
      <div class="small" id="att_meta"></div>
      <div id="att_plot" style="width:100%; height:520px; margin-top:10px;"></div>
      <div class="note" id="att_note"></div>
    </div>


 

<hr>

<h3 style="margin: 0 0 8px;">Feed-forward activity (heatmap)</h3>
<p class="small" style="margin-top:0;">
  After attention, each layer updates <b>how strongly it processes each token on its own</b>.<br>
  This heatmap shows how active the feed-forward part of the model is for each token at each step.
  <br><br>
  Attention decides which tokens influence each other.
  The feed-forward layers focus on refining each token’s meaning individually.
</p>

<button class="btn" onclick="toggleFFN()">Reveal FFN heatmap</button>

<div id="ffn_block" class="reveal" style="display:none;">
  <div class="small" id="ffn_meta"></div>
  <div id="ffn_plot" style="width:100%; height:520px; margin-top:10px;"></div>
  <div class="note" id="ffn_note"></div>
<div class="note">
  <b>How to read this heatmap:</b><br><br>

  <b>Early layers:</b> The model is recognizing basic things, like words, numbers, and symbols.<br><br>

  <b>Middle layers:</b> The model does the most work here, combining meaning and context
(for example, understanding that “-1.0%” means a negative change).<br><br>

  <b>Upper layers:</b> The model settles on an interpretation so it can make a decision.
</div>
</div>

<hr>

<h3 style="margin: 0 0 8px;">Model summary</h3>

<button class="btn" onclick="toggleFlow()" aria-expanded="false">
  How the model makes its decision ▾
</button>

<div id="flow_block" class="reveal" style="display:none; margin-top:12px;">
  <p class="small" style="margin-top:0;">
    This is the same process for every sentence. Only the numbers change.
  </p>

  <ul style="margin:10px 0 0 18px;">
    <li><strong>Sentence</strong> – the original text</li>
    <li><strong>Tokens</strong> – split into words and symbols</li>
    <li><strong>Embeddings</strong> – numbers that place tokens in a “meaning space”</li>
    <li><strong>Attention</strong> – which tokens matter to each other</li>
    <li><strong>Feed-forward layers</strong> – refine each token’s meaning</li>
    <li><strong>Final scoring layer</strong> – assign scores to each label</li>
    <li><strong>Softmax</strong> – turn scores into probabilities</li>
    <li><strong>Output</strong> – Positive / Neutral / Negative</li>
  </ul>
</div>

  <hr>

  <h3 style="margin: 0 0 8px;"> Sensitivity check: which words drive the result? (remove-one-token test)</h3>
  <p class="small" style="margin-top:0;">
    We remove one token at a time, decode back to text, and rerun FinBERT.<br><br>
    Color = top label after removal (red/blue/grey).<br>
    Brightness = top probability.
  </p>

  <button class="btn" onclick="toggleRemoval()">Reveal token removal heatmap</button>

  <div id="mask_block" class="reveal" style="display:none;">
    <div class="small" id="mask_meta"></div>
    <div id="mask_plot" style="width:100%; height:260px; margin-top:10px;"></div>
    <div class="note" id="mask_note"></div>
  </div>
    <!-- /Masking (Token Impact) Heatmap Block -->

  </div>
</div>

<script>
function reveal(id){
  const el = document.getElementById(id);
  if(!el) return;
  el.style.display = (el.style.display === "block") ? "none" : "block";
}

function toggleEmbeddings(){
  const el = document.getElementById("emb_block");
  const isHidden = (el.style.display === "none" || el.style.display === "");
  el.style.display = isHidden ? "block" : "none";

  // load embeddings only once, the first time user opens it
  if (isHidden && !window.__embLoaded) {
    window.__embLoaded = true;
    loadEmbeddingsB().catch(err => {
      document.getElementById("emb_table").innerHTML =
        "<p style='color:#a00;'>Failed to load embeddings_sentenceB.json. Put it next to index.html and serve via http:// (not file://).</p>";
      console.error(err);
    });
  }
}

  async function loadEmbeddingsB() {
    const res = await fetch("embeddings_sentenceB.json", { cache: "no-store" });
    if (!res.ok) throw new Error(`HTTP ${res.status} loading embeddings_sentenceB.json`);
    const data = await res.json();

    // meta line
    document.getElementById("emb_meta").textContent =
      `Model: ${data.model} | Tokens: ${data.rows?.length ?? 0}`;

    // build table
    const rows = data.rows || [];
    let html = `
      <table class="tbl" aria-label="token-embeddings">
        <thead>
          <tr>
            <th style="width:160px;">token</th>
            <th>embedding (first 4 of 768 dims)</th>
          </tr>
        </thead>
        <tbody>
    `;

    for (const r of rows) {
      const tok = String(r.token ?? "");
      const vec = (r.embedding_preview || []).map(v => Number(v).toFixed(3));
      const vecStr = `[ ${vec.join(", ")}, … ]`;

      const isMinus = (tok === "-");
      html += `
        <tr class="${isMinus ? "hlRow" : ""}">
          <td><code>${tok.replace(/</g,"&lt;").replace(/>/g,"&gt;")}</code></td>
          <td><code>${vecStr}</code></td>
        </tr>
      `;
    }

    html += `</tbody></table>`;
    document.getElementById("emb_table").innerHTML = html;
  }


  // Attention loading + rendering (expects attention.json in the same folder)
  async function loadAttention() {
    const res = await fetch("attention.json", { cache: "no-store" });
    if (!res.ok) throw new Error(`HTTP ${res.status} loading attention.json`);
    const data = await res.json();

    document.getElementById("att_sentence").textContent = data.sentence || "";
    document.getElementById("att_meta").textContent =
      `Model: ${data.model} | Layer: ${data.layer} | Head: ${data.head} | Tokens: ${data.tokens?.length ?? 0}`;

    document.getElementById("att_note").innerHTML = data.note || "";

    const traceHeat = {
      type: "heatmap",
      z: data.attention,
      x: data.tokens,
      y: data.tokens,
      hovertemplate: "From: %{y}<br>To: %{x}<br>Attn: %{z:.4f}<extra></extra>"
    };

    Plotly.newPlot(
      "att_plot",
      [traceHeat],
      {
        margin: { l: 120, r: 20, t: 10, b: 140 },
        xaxis: { tickangle: 45, automargin: true },
        yaxis: { automargin: true },
        height: 520
      },
      { displayModeBar: false, responsive: true }
    );
  }

  function toggleAttention() {
    const el = document.getElementById("att_block");
    const isHidden = (el.style.display === "none" || el.style.display === "");
    el.style.display = isHidden ? "block" : "none";

    if (isHidden && !window.__attLoaded) {
      window.__attLoaded = true;
      loadAttention().catch(err => {
        document.getElementById("att_plot").innerHTML =
          "<p style='color:#a00;'>Failed to load attention.json. Make sure it is in the same folder as index.html and served via http:// (not file://).</p>";
        console.error(err);
      });
    }
  }


  // Removal loading + rendering (expects removal.json in the same folder)
 async function loadRemoval() {
  const res = await fetch("removal.json", { cache: "no-store" });
  if (!res.ok) throw new Error(`HTTP ${res.status} loading removal.json`);
  const data = await res.json();

  const results = data.results || [];
  const n = results.length;

  const tokens = results.map(r => String(r.removed_token ?? ""));
  const labels = results.map(r => String(r.top_label_after_removal ?? ""));
  const probs  = results.map(r => Number(r.top_prob_after_removal ?? 0));

  function colorFor(label, conf) {
    const l = String(label || "").toLowerCase();
    const c = Math.max(0, Math.min(1, Number(conf) || 0)); // clamp 0..1

    let base;
    if (l === "negative") base = [220, 38, 38];       // red
    else if (l === "positive") base = [37, 99, 235];  // blue
    else base = [150, 150, 150];                      // grey

    const rr = Math.round(base[0] * c);
    const gg = Math.round(base[1] * c);
    const bb = Math.round(base[2] * c);
    return `rgb(${rr},${gg},${bb})`;
  }

  // colored rectangles as shapes (prevents label overlap)
  const shapes = tokens.map((_, i) => ({
    type: "rect",
    xref: "x",
    yref: "y",
    x0: i - 0.5,
    x1: i + 0.5,
    y0: 0,
    y1: 1,
    fillcolor: colorFor(labels[i], probs[i]),
    line: { width: 0 }
  }));

  // token labels below the bar
const annotations = tokens.map((t, i) => ({
  x: i - 0.2,          // ← shift right
  y: -0.35,
  xref: "x",
  yref: "y",
  text: String(t).replace(/</g, "&lt;").replace(/>/g, "&gt;"),
  showarrow: false,
  textangle: 45,
  xanchor: "left",      // ← anchor on left side
  font: { size: 12, color: "#111" }
}));

  // invisible hover targets
  const hoverTrace = {
    type: "scatter",
    mode: "markers",
    x: [...Array(n).keys()],
    y: Array(n).fill(0.5),
    marker: { size: 40, opacity: 0 },
    customdata: results.map(r => ([
      r.removed_token,
      r.top_label_after_removal,
      r.top_prob_after_removal,
      r.text_after_removal,
      r.probs_after_removal?.positive,
      r.probs_after_removal?.negative,
      r.probs_after_removal?.neutral
    ])),
    hovertemplate:
      "<b>Removed token</b>: %{customdata[0]}<br>" +
      "<b>Top label</b>: %{customdata[1]}<br>" +
      "<b>Top prob</b>: %{customdata[2]:.3f}<br>" +
      "<b>Text after removal</b>: %{customdata[3]}<br>" +
      "P(pos): %{customdata[4]:.3f} | P(neg): %{customdata[5]:.3f} | P(neu): %{customdata[6]:.3f}" +
      "<extra></extra>",
    showlegend: false
  };

  document.getElementById("mask_meta").textContent =
    `Model: ${data.model} | Method: token removal (decode + rerun) | Original: ${data.original?.top_label} (${(data.original?.top_prob ?? 0).toFixed(3)})`;

  document.getElementById("mask_note").textContent =
    `Red = Negative | Blue = Positive | Grey = Neutral | Brightness = top probability`;

  const layout = {
    shapes,
    annotations,
    xaxis: {
      range: [-0.5, n - 0.5],
      showgrid: false,
      zeroline: false,
      showticklabels: false
    },
yaxis: {
  range: [-0.55, 1.05],   // <- more space below
  showgrid: false,
  zeroline: false,
  showticklabels: false
},
margin: { l: 80, r: 20, t: 10, b: 150 } // <- more bottom padding
  };

  Plotly.newPlot("mask_plot", [hoverTrace], layout, {
    displayModeBar: false,
    responsive: true
  });
}

function toggleRemoval() {
  const el = document.getElementById("mask_block");
  const isHidden = (el.style.display === "none" || el.style.display === "");
  el.style.display = isHidden ? "block" : "none";

  if (isHidden && !window.__removalLoaded) {
    window.__removalLoaded = true;
    loadRemoval().catch(err => {
      document.getElementById("mask_plot").innerHTML =
        "<p style='color:#a00;'>Failed to load removal.json. Make sure it is in the same folder as index.html and served via http:// (not file://).</p>";
      console.error(err);
    });
  }
}

function toggleFlow(){
  const el = document.getElementById("flow_block");
  const btn = event.currentTarget;

  const isHidden = (el.style.display === "none" || el.style.display === "");
  el.style.display = isHidden ? "block" : "none";

  btn.textContent = isHidden
    ? "How the model makes its decision ▴"
    : "How the model makes its decision ▾";

  btn.setAttribute("aria-expanded", isHidden);
}

async function loadFFN() {
  const res = await fetch("ffn_heatmap_sentenceB.json", { cache: "no-store" });
  if (!res.ok) throw new Error(`HTTP ${res.status} loading ffn_heatmap_sentenceB.json`);
  const data = await res.json();

  document.getElementById("ffn_meta").textContent =
    `Model: ${data.model} | Layers: ${data.layers?.length ?? 0} | Tokens: ${data.tokens?.length ?? 0}`;

  document.getElementById("ffn_note").textContent = data.note || "";

  const trace = {
    type: "heatmap",
    z: data.heat,      // [layers][tokens]
    x: data.tokens,
    y: data.layers,
    hovertemplate: "Layer: %{y}<br>Token: %{x}<br>FFN (log-scaled): %{z:.4f}<extra></extra>"
  };

  Plotly.newPlot(
    "ffn_plot",
    [trace],
    {
      margin: { l: 70, r: 20, t: 10, b: 140 },
      xaxis: { tickangle: 45, automargin: true },
      yaxis: { automargin: true },
      height: 520
    },
    { displayModeBar: false, responsive: true }
  );
}

function toggleFFN() {
  const el = document.getElementById("ffn_block");
  const isHidden = (el.style.display === "none" || el.style.display === "");
  el.style.display = isHidden ? "block" : "none";

  if (isHidden && !window.__ffnLoaded) {
    window.__ffnLoaded = true;
    loadFFN().catch(err => {
      document.getElementById("ffn_plot").innerHTML =
        "<p style='color:#a00;'>Failed to load ffn_heatmap_sentenceB.json. Make sure it is next to index.html and served via http:// (not file://).</p>";
      console.error(err);
    });
  }
}


</script>


<hr>

<h2 style="margin-top:10px;">Part III — Large Language Model (Llama)</h2>
<p class="small" style="margin-top:4px;">
We now switch to a large language model (Llama, ~8B parameters).
<br>
Unlike a classifier, it generates text and can be instructed to adopt a specific role and audience.
</p>

 <!-- 4) Role / instruction clarity (Llama) -->
<div class="card" id="q4">
  <div class="pill">4/5 — Roles in LLM (Llama-3.1-8B-Instruct)</div>
  <h2>Does role instruction change interpretation?</h2>

  <div class="sentence">
    Sentence: “Inflation remains uncomfortably high.”
  </div>

  <div class="small" style="margin-top:6px; line-height:1.5;">
    <strong>Question 4:</strong>
    If we instruct Llama to act as a <strong>Senior ECB economist writing a briefing note for the Governor</strong>,
    does its interpretation change?
  </div>

  <div class="options">
    <label class="opt"><input type="radio" name="roleQ" value="Yes">Yes</label>
    <label class="opt"><input type="radio" name="roleQ" value="No">No</label>
  </div>

  <button class="btn" onclick="reveal('roleOut')">Reveal Llama prompts + outputs</button>

  <div class="reveal" id="roleOut">
    <div class="grid2">
      <div class="promptBlock">
        <div class="promptHdr">
          <div class="title">Llama — Prompt 1 (generic)</div>
          <span class="tag">USER only</span>
        </div>

        <div class="mono"><strong>USER</strong>
Classify the tone of the following sentence (Positive, Neutral, or Negative) and explain:

"Inflation remains uncomfortably high."</div>

        <div class="divider"></div>

        <div class="mono"><strong>OUTPUT</strong>
The tone of the sentence "Inflation remains uncomfortably high" is <strong>Negative</strong>. 

This is because the word "uncomfortably" implies that the speaker is expressing concern or dissatisfaction with the current state of inflation, suggesting that it is a problem or an issue that needs to be addressed. The use of the word "uncomfortably" adds a sense of unease or discomfort, which is a negative emotional connotation.</div>
      </div>

      <div class="promptBlock">
        <div class="promptHdr">
          <div class="title">Llama — Prompt 2 (role + audience)</div>
          <span class="tag">SYSTEM + USER</span>
        </div>

        <div class="mono"><strong>SYSTEM</strong>
You are a Senior ECB economist writing a briefing note for the Central Bank Governor. 
Your task is to classify the tone (Positive, Neutral, Negative) of the sentence and also report policy implications.</div>

        <div class="divider"></div>

        <div class="mono"><strong>USER</strong>
Classify the tone of the following sentence (Positive, Neutral, or Negative) and explain:

"Inflation remains uncomfortably high"</div>

        <div class="divider"></div>

        <div class="mono"><strong>OUTPUT</strong>
The tone of the sentence is <strong>Negative.</strong>

Policy implications: The Central Bank may consider tightening monetary policy to combat high inflation, potentially increasing interest rates to curb price growth.</div>
      </div>
    </div>

    <div class="note">
      <div class="note">
  Separating SYSTEM and USER roles reflects how chat-based models are trained.
  The SYSTEM message defines behavior and context (role, audience, style),
  while the USER message defines the task.
  Keeping them distinct can improve instruction stability and clarity.
</div>
    </div>
  </div>
</div>

<!-- 5) From role to agent with validation (Llama runs multiple times) -->
<div class="card" id="q5">
  <div class="pill">5/5 — From Role to Agent</div>
  <h2>What if Llama validates its own answer before finalizing?</h2>

  <div class="sentence">
    Objective: Produce a Governor-ready ECB briefing note.
  </div>

  <div class="small" style="margin-top:6px; line-height:1.5;">
    In Part III, <strong>Llama</strong> produced a single answer from one prompt.<br>
    In an <strong>AI agent</strong> setup, we use <strong>Llama multiple times</strong> in a loop:
    first to draft, then for <strong>validation</strong>, and (if needed) to revise.
  </div>

  <div class="note" style="margin-top:10px;">
    <strong>Where Llama is used:</strong><br>
    (1) Draft → (2) Validation → (3) Revision (only if validation fails)
  </div>
<br>
  <button class="btn" onclick="reveal('agentOut')">Reveal step-by-step (Agent loop)</button>

  <div class="reveal" id="agentOut">
    <div class="grid2">

      <div class="promptBlock">
        <div class="promptHdr">
          <div class="title">Agent Step A: Draft (Llama call #1)</div>
          <span class="tag">SYSTEM + USER</span>
        </div>

        <div class="mono"><strong>SYSTEM</strong>
You are a Senior ECB economist writing a briefing note for the Governor.
Be formal, cautious, and policy-relevant.</div>

        <div class="divider"></div>

        <div class="mono"><strong>USER</strong>
Sentence: “Inflation remains uncomfortably high.”
Tone: Negative
Policy stance guidance: Emphasise vigilance; inflation risks remain salient; communication should stay restrictive/firm and data-dependent.

Draft the briefing using this template:
Tone: &lt;Positive|Neutral|Negative&gt;
Policy implication: &lt;1-2 sentences&gt;
Briefing note: &lt;3-5 sentences, governor-ready&gt;</div>

        <div class="divider"></div>

        <div class="mono"><strong>OUTPUT (example)</strong>
Tone: Negative

Policy implication: Persistent inflationary pressures warrant continued vigilance and a restrictive, data-dependent communication stance.
Briefing note: The sentence signals concern that inflation remains elevated. This suggests price pressures are still inconsistent with comfort levels. Communication should reinforce the priority of price stability and maintain a risk-management approach. Policy should remain guided by incoming data and inflation persistence.</div>
      </div>

      <div class="promptBlock">
        <div class="promptHdr">
          <div class="title">Agent Step B: Validation (Llama call #2)</div>
          <span class="tag">SYSTEM + USER</span>
        </div>

        <div class="mono"><strong>SYSTEM</strong>
You are a Senior ECB economist. Validate the draft for clarity and constraints.
Be strict and specific. Reply with:
VALID: Yes/No
ISSUES: bullet list (if any)
REVISED: (only if VALID=No)</div>

        <div class="divider"></div>

        <div class="mono"><strong>USER</strong>
Validate this draft:
[PASTE DRAFT OUTPUT HERE]

Constraints:
- Must include lines starting with “Tone:” and “Policy implication:”
- Must be concise (max ~110 words)
- Must be Governor-ready (formal, cautious)</div>

        <div class="divider"></div>

        <div class="mono"><strong>OUTPUT (example)</strong>
VALID: Yes
ISSUES: None</div>
      </div>

    </div>

    <div class="note">
      If <strong>validation fails</strong> (e.g., too long or missing required fields), the agent calls <strong>Llama again</strong> to revise:
      <br><br>
      <strong>Llama call #3 (optional):</strong> Revision → then re-validation until constraints pass.
    </div>
  </div>
</div>

  <hr>
  <p class="small">
    <strong>Closing note:</strong> These examples illustrate sensitivity to minor textual changes, hedging language, and instruction framing.
    <br>
    <strong>Robustness checks</strong> are essential for policy-facing NLP applications.
  </p>


<div class="small" style="margin-top:20px;">
  <strong>About this project</strong><br>
  This interactive demonstration was designed and implemented by Joni H.
  It illustrates how transformer-based NLP models behave under different inputs,
  tokenization changes, and instruction framing.
</div>


<footer class="small" style="margin-top:40px; border-top:1px solid #ddd; padding-top:14px;">
  © 2026 Joni H.  
  All code and visualizations created by the author.  
  FinBERT: ProsusAI | Llama: Meta.
</footer>

</div>  
</body>
</html>
